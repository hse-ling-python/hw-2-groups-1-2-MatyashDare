{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ДЗ 2**\n",
    "\n",
    "**Матяш Дарья БКЛ-181**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала включим проверку PEP8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем все нужные модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pymystem3 import Mystem\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Парсим\" текст с помощью Mystem и замеряем время работы\n",
    " **Важно!** я клянусь, у меня ВСЁ РАБОТАЛО, А ПОТОМ РЕЗКО текст тхт перестал восприниматься как что-то нормальное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "with open('groza.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    start = time.time()\n",
    "    ana = m.analyze(text)\n",
    "    end = time.time()\n",
    "    print('Время работы составило: ', end - start, 'сек.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем результат в json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(ana, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем текст с помощью nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизируем текст, приводим к нижнему регистру\n",
    "# и оставляем только последовательности из букв,\n",
    "# т.е. все токены, где были знаки препинания и числа, исчезнут\n",
    "words = [w.lower() for w in word_tokenize(text) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбираем слова с помощью pymorphy и замеряем время работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "ans_time = 0\n",
    "for i in range(len(words)):\n",
    "    start = time.time()\n",
    "    morph.parse(words[i])\n",
    "    end = time.time()\n",
    "    dif = end - start\n",
    "    ans_time += dif\n",
    "print('Время работы составило: ', ans_time, 'сек.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_d = {}\n",
    "for i in range(len(words)):\n",
    "    w = morph.parse(words[i])[0]\n",
    "    ans_d[w.normal_form] = w.tag.POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем результат в json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_2.json', 'w', encoding='utf-8') as f_2:\n",
    "    json.dump(ans_d, f_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Какую долю слов составляет каждая часть речи?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = len(words)\n",
    "ans_pos = {}\n",
    "verbs = []\n",
    "adverbs = []\n",
    "for i in range(len(words)):\n",
    "    w = morph.parse(words[i])[0]\n",
    "    if w.tag.POS in ans_pos:\n",
    "        ans_pos[w.tag.POS] += 1\n",
    "    else:\n",
    "        ans_pos[w.tag.POS] = 1\n",
    "    if w.tag.POS == 'VERB':\n",
    "        verbs.append(w.normal_form)\n",
    "    elif w.tag.POS == 'ADVB':\n",
    "        adverbs.append(w.normal_form)\n",
    "        verbs.append(w.normal_form)\n",
    "print('Доля части речи в тексте', '\\n')\n",
    "print('Часть речи:', '\\t', 'Её доля в тексте:', '\\n')\n",
    "for key in ans_pos.keys():\n",
    "    print(key, '\\t', ans_pos[key] / number_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Топ-20 (по частотности) глаголов и наречий**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_verbs = Counter(verbs).most_common(20)\n",
    "print('Топ-20 частотных глаголов:', '\\n')\n",
    "count = 1\n",
    "for tup in top_10_verbs:  # выводим топ-10 глаголов\n",
    "    print(str(count) + ')', '\\t', tup[0])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_adverbs = Counter(adverbs).most_common(20)\n",
    "print('Топ-20 частотных наречий:', '\\n')\n",
    "count = 1\n",
    "for tup in top_10_adverbs:\n",
    "    print(str(count) + ')', '\\t', tup[0])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Топ-25 биграмм и триграмм**\n",
    "\n",
    "**Примечание:** я считала би- и три-граммы именно по леммам, а не про формам, представленным в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lemmas = []\n",
    "for w in ana:   # ходим по размеченному тексту\n",
    "    if 'analysis' in w.keys():\n",
    "        if w['analysis'] != []:\n",
    "            lemma = w['analysis'][0]['lex']\n",
    "            words_lemmas.append(lemma)   # сохраняем в список все леммы\n",
    "bigram = list(nltk.bigrams(words_lemmas))   # создаем список в биграммами\n",
    "top_25_bigrams = Counter(bigram).most_common(25)\n",
    "count = 1\n",
    "print('Топ-25 биграм:', '\\n')\n",
    "for tup in top_25_bigrams:\n",
    "    print(tup)\n",
    "    print(str(count) + ')', '\\t', list(tup[0])[0], '\\t', list(tup[0])[1])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = list(nltk.bigrams(words_lemmas))   # создаем список в биграммами\n",
    "top_25_bigrams = Counter(bigram).most_common(25)\n",
    "count = 1\n",
    "for tup in top_25_bigrams:\n",
    "    print(tup)\n",
    "    print(str(count) + ')', '\\t', list(tup[0])[0], '\\t', list(tup[0])[1])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = list(nltk.trigrams(words_lemmas))  # создаем список с триграммами\n",
    "top_25_trigrams = Counter(trigram).most_common(25)\n",
    "count = 1\n",
    "print('Топ-25 триграм:', '\\n')\n",
    "for tup in top_25_trigrams:  # выводим топ-25 биграм\n",
    "    list_tup = list(tup[0])\n",
    "    print(str(count) + ')', *list_tup[:2])\n",
    "    count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
